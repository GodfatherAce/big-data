Assignment 2 - Hadoop Streaming
---------------------------------------

All the tasks should be done using Hadoop Streaming (Hadoop version 2.x) and Python.
The input files (Wikipedia articles) are available at http://bigdata.poly.edu/~fchirigati/mda-class/hadoop-streaming/articles.zip
The mapper is available at http://bigdata.poly.edu/~fchirigati/mda-class/hadoop-streaming/map.py
The reducer is available at http://bigdata.poly.edu/~fchirigati/mda-class/hadoop-streaming/reduce.py

Do not forget to make the Python scripts executable by running:

  chmod +x map.py reduce.py

otherwise Hadoop Streaming may not work.

Also, when running Hadoop Streaming, make sure you only use a single reducer, i.e., make sure you use the following configuration parameter:

  -D mapreduce.job.reduces=1

The command line for Hadoop Streaming looks like the following:

  hadoop jar path-to-hadoop-streaming-jar -D mapreduce.job.reduces=1 -file map.py -mapper map.py -file reduce.py -reducer reduce.py -input path-to-input -output path-to-output

To locate where the Hadoop Streaming jar is stored in your computer, use the following command line:

  locate hadoop-streaming

Note that the command line may change depending on your MapReduce job.

Please submit a .zip file with the following structure:
  - One directory per task, named "taskX", where X is the number of the task.
  - Inside each directory, please include:
    - The Python scripts for the map and reduce phases;
    - The output directory generated by Hadoop;
    - Any other files or scripts used by your Hadoop job.
  - Please DO NOT submit the input files (Wikipedia articles).

===================================================================

Assignment - Local Hadoop Streaming

- Task 1 (CleanWordCount):
  - Modify the WordCount example to remove stop words, punctation characters, and numbers.
    A list of stop words can be found here: http://xpo6.com/wp-content/uploads/2015/01/stop-word-list.csv
    To remove the punctation, take a look at the string module in Python: https://docs.python.org/2/library/string.html
  - Output: A key-value pair per line, where key is the word, and value is the number of times the word appears in the input.
  - The output directory produced by Hadoop should be named CleanWordCount.

- Task 2 (InitialCount):
  - Modify CleanWordCount to count the number of words based on their initial (first character), i.e., count the number of words per initial.
  - The letter case should not be taken into account. For example, Apple and apple will be both counted for initial A.
  - Output: A key-value pair per line, where key is the initial (in UPPERCASE), and value is the number of words having that initial (in either uppercase or lowercase).
  - The output directory produced by Hadoop should be named InitialCount.

- Task 3 (Top-K WordCount):
  - Modify CleanWordCount to output the top 100 most frequent 7-character words, in descending order of frequency.
  - Output: A key-value pair per line, where key is the word, and value is the number of times the word appears in the input.
  - The output directory produced by Hadoop should be named TopK.

- Task 4 (InvertedIndex):
  - Modify CleanWordCount to produce an inverted index.
  - An inverted index contains, for each word, the names of the documents (i.e., files) that contain that word, and the frequency for each document. Thus, if the word "nyu" appears in documents 0010 (50 times) and 0090 (60 times), the output should be:
    nyu    0010 50, 0090 60
  - Output: A key-value pair per line, where key is the word, and value is the list of documents that contain that word (together with the frequency), separated by comma. For each key, sort the list of documents in ascending order of frequency; if more than two documents have the same frequency, use the name of the document as a tie-breaker. For instance, if "cds" appears in documents 0010 (50 times), 0000 (50 times), and 0063 (35 times), the output should be:
    cds    0063 35, 0000 50, 0010 50
  - The output directory produced by Hadoop should be named InvertedIndex.
  - Hint: During Hadoop execution, the name of the file being read by a mapper is stored in the "mapreduce_map_input_file" environment variable. In Python, you can use the os.environ to get the value of an environment variable: https://docs.python.org/2/library/os.html#os.environ